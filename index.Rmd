---
title: "Practical Machine Learning Course Project"
author: "Marc Paelinck"
date: "September 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

##1. Analyzing and preprocessing the data
### Load the Caret package and read the data
```{r readdata}
library(caret)
training <- read.csv("D:/Documents/Cursussen/Coursera/Practical Machine Learning/Week4/pml-training.csv", na=c("#DIV/0!","","NA"))
testing <- read.csv("D:/Documents/Cursussen/Coursera/Practical Machine Learning/Week4/pml-testing.csv")
```
Note that I translate the value `#DIV/0!` into N/A. This value occurs in one of the columns and might otherwise cause problems.

###Split the data
The pml-testing.csv file only contains 20 observations and doesn't contain a classe column, so it is not suited to evaluate the out of sample error. We will therfore split a validation data set from the training data.
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.75, list=FALSE)
training.train = training[inTrain,]
training.test = training[-inTrain,]
```
It is good practice to only use the training data set for data analysis. So we will use the `training.train` data set in the following analysis.

### First columns differ from the rest
The first seven columns seem to contain a different type of data than the remaining columns.
```{r summary}
summary(training.train[,1:7])
```
The first column a row numbering, which we can check with the following statement.
```{r checkX}
sum(training.train$X==as.numeric(row.names(training.train))) == nrow(training.train)
```

The tested person's name is not a suitable predictor, this would make the model too specific (only applicable for the tested persons).  
The date stamp might be useful if we wanted to use the progress of a person as a predictor. However discarding this information would make the model more generic by also enabling to make a prediction for persons without any measuring history, so for the time being we will discard all columns containing date and time window information.
```{r dropColumns}
training.train <- training.train[,-c(1:7)]
```

### Columns with large number of N/A values
Next, it appears that a large number of columns contain N/A values. We will plot a histogram in order to find out how these values are distributed over the columns.
```{r histogram}
NA_values <- sapply(training.train, function(x) sum(is.na(x)))/nrow(training.train)
histogram(NA_values, nint=10, xlab="% of N/A values in a column", ylab="Percent of total columns")
```
  
More than 60% of the columns have over 90% N/A values. We will remove these columns.
```{r removeNA}
training.train <- training.train[,NA_values<.5]
```

### Correlated columns
Create a covariance matrix and set the lower triangular part of the matrix to zero in order to remove duplicate column pairs.
```{r corrMatrix}
M<-abs(cor(training.train[,-which(names(training.train)==c("classe"))]))
M[lower.tri(M,diag=TRUE)] <- 0
```

Now make a list of the most correlated columns
```{r corrVars}
select <- data.frame(which(M>0.75, arr.ind=T), row.names=NULL)
covar <- data.frame(cbind(col1=rownames(M[select$row,]), col2=colnames(M[,select$col])))
covar$cov <- M[which(M>0.75, arr.ind=T)]
covar <- covar[order(-covar[,3]),]
covar
```
Note that not all models are sensitive to covariated variables, so it is possible that we will not need this information.


## 2. Modeling
### Set the seed 
```{r seed}
set.seed(34654)
```

We will try several models and will keep track of the computer time used to train each model.

### Tree model
We will first try a tree model. This model is relatively efficient in terms of computer resource usage.
```{r treeModel}
startClock <- proc.time() # Time the training session
modelFitTree <- train(classe ~., method="rpart", data=training.train)
durationTree <- proc.time() - startClock  
confusionTree <- table(predict=predict(modelFitTree, training.test), actual=training.test$classe)
print(durationTree)
confusionMatrix(confusionTree)
```
This model doesn't perform well. The confusion matrix shows that the model has a tendency to predict a 'better' class than the actual one. 

### Random forest model
A first trial using the entire `training.train` dataset gave problems with memory usage, so we will only use half of the `training.train` data. We will keep track of the calculation time.

```{r randomForest1, results='hide'}
inTrain50 <- createDataPartition(y=training.train$classe, p=0.30, list=FALSE)
training.train50 = training.train[inTrain50,]
startClock <- proc.time() # Time the training session
modelFitForest<-train(classe ~.,data=training.train50 ,method="rf",
                trControl=trainControl(method="cv",number=5),
                prox=TRUE, allowParallel=TRUE)
durationForest <- proc.time() - startClock  
```
```{r randomForest2}
confusionForest <- table(predict=predict(modelFitForest, training.test), actual=training.test$classe)
print(durationForest)
confusionMatrix(confusionForest)
```
This model gives an impressively high out of sample accuracy given that we only used 37.5% of the data for training (half of the training set which consists of 75% of the data).

### Boosting model
Out of curiosity I will also try a boosting model: the Stochastic Gradient Boosting model which is one of the models mentioned in the course. I would like to know if this model performs better in terms of calculation time. For the comparison we will use the same training dataset and model settings. 

```{r boosting1, results='hide'}
startClock <- proc.time() # Time the training session
modelFitGBM <- train(classe ~., method="gbm", data=training.train50, 
                     trControl=trainControl(method="cv",number=5),
                     verbose=FALSE)
durationGBM <- proc.time() - startClock
```
```{r boosting2}
confusionGBM <- table(predict=predict(modelFitGBM, training.test), actual=training.test$classe)
print(durationGBM)
confusionMatrix(confusionGBM)
```

Contrary to the Random Forest model, this model can also cope with the entire `training.train` dataset. 

```{r boostingAll1, results='hide'}
startClock <- proc.time() # Time the training session
modelFitGBMall <- train(classe ~., method="gbm", data=training.train, 
                     trControl=trainControl(method="cv",number=5),
                     verbose=FALSE)
durationGBMall <- proc.time() - startClock
```
```{r boostingAll2}
confusionGBMall <- table(predict=predict(modelFitGBM, training.test), actual=training.test$classe)
print(durationGBMall)
confusionMatrix(confusionGBMall)
```

### Conclusion
The following table compares the models.

``` {r conclusions, echo=FALSE}
accTree <- sum(diag(confusionTree))/sum(confusionTree)
accForest <- sum(diag(confusionForest))/sum(confusionForest)
accGBM <- sum(diag(confusionGBM))/sum(confusionGBM)
accGBMall <- sum(diag(confusionGBMall))/sum(confusionGBMall)
```

Â | Tree | Random Forest | Stochastic Gradient Boosting | SGB (entire training.train dataset)
---------|------|---------------|----------------------|------
Accuracy | `r accTree` | `r accForest` | `r accGBM`  | `r accGBMall`
Calculation time | `r durationTree[3]` | `r durationForest[3]` | `r durationGBM[3]` | `r durationGBMall[3]`

The Random Forest model performs best, but training takes a relatively long time. The SGB using the entire `training.train` dataset performs almost as well as the Random Forest model but takes about the same time for training. Using only half of the training dataset does not affect the performance of the boosting model, while it reduces its training time by almost 75%. 
The tree model performs poorly and is not a good option for our case.

